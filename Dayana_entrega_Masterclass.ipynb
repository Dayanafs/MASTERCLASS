{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8bcxFX85UO"
      },
      "source": [
        "# Práctica Final: Detección de mensajes Troll\n",
        "\n",
        "## Realizada por:**DAYANA FRANCO**\n",
        "\n",
        "**Objetivo:**\n",
        "\n",
        "Diseñar una Inteligencia Artificial que pueda identificar y NOTROLLizar  mensajes ofensivos recibidos en Twitch, también se implementará para inferencias en batch, estableciendo un nuevo estándar en la industria. El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgE9XB8ACVmQ"
      },
      "source": [
        "# 1. Configuración de nuestro proyecto en GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAorVw7RCT9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab1537d-05cd-4d72-ce20-f095807f551f"
      },
      "source": [
        "PROJECT_ID = \"aesthetic-cacao-398815\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbAl8pSkCXCm"
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el bucket mediante la consola y una vez creado lo indicamos en la variable:"
      ],
      "metadata": {
        "id": "p5Xoej7vH0X-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Zpx14FCfXy"
      },
      "source": [
        "BUCKET_NAME = \"bucket-dayana\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE4V-Xt0ChSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122b0bc4-f89a-471f-93ad-084442be65ba"
      },
      "source": [
        "! gsutil ls -al \"gs://bucket-dayana\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 gs://bucket-dayana//\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8Qw14y_nRa"
      },
      "source": [
        "# Entrenamiento e inferencia en Batch\n",
        "\n",
        "## Preparación\n",
        "\n",
        "Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**, es decir, lo importante no es que nuestro modelo detecte correctamente los tweets de trolls si no que funcione y sea capaz de hacer inferencias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_dh-471gIq"
      },
      "source": [
        "A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región. Os dejo disponibilizado el dataset en un bucket de acceso público:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqp4L_nmUjAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e76310-7aa1-4bba-f3ca-e3edc843f672"
      },
      "source": [
        "%pip install gdown\n",
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\n",
            "To: /content/dataset-cybertrolls.json\n",
            "100% 2.76M/2.76M [00:00<00:00, 14.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCQQ9j2I11tg"
      },
      "source": [
        "Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/batch"
      ],
      "metadata": {
        "id": "sPSF3DYFsQLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK51quU1_oi"
      },
      "source": [
        "Se establece el directorio de trabajo que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ybSaotRwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58cd30e-cd77-4641-c922-6d0b88bac1df"
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd /content/batch\n",
        "\n",
        "WORK_DIR = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSd4bAo2aj_"
      },
      "source": [
        "Ahora se descargarán los datos en el workspace de Colab para trabajar en local."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\"\n",
        "! mv dataset-cybertrolls.json dataset.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEtww3jSIRAA",
        "outputId": "3347850d-3c0f-46b9-843e-932915d671df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\n",
            "To: /content/batch/dataset-cybertrolls.json\n",
            "\r  0% 0.00/2.76M [00:00<?, ?B/s]\r 38% 1.05M/2.76M [00:00<00:00, 10.3MB/s]\r100% 2.76M/2.76M [00:00<00:00, 20.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoW6zyg2kEz"
      },
      "source": [
        "Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIl8lrPp2x2j"
      },
      "source": [
        "Instalamos las dependencias. **No olvides reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "\n",
        "tensorflow\n",
        "gensim\n",
        "fsspec\n",
        "gcsfs\n",
        "numpy\n",
        "apache-beam[gcp]\n",
        "argparse\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OAVRbkXKB2m",
        "outputId": "16a260b3-5047-453d-9cf0-41d8236ea79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m0bypg3SK1zu",
        "outputId": "cfe147b0-625b-4412-f3b2-449494a20099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.13.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.3.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2023.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.23.5)\n",
            "Requirement already satisfied: apache-beam[gcp] in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.50.0)\n",
            "Collecting argparse (from -r requirements.txt (line 9))\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.33.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->-r requirements.txt (line 4)) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->-r requirements.txt (line 4)) (6.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (3.8.5)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (1.7)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (3.9.7)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (1.8.3)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (0.18)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.7.2)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (0.22.0)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (0.6.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (4.5.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (1.22.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2023.3.post1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2023.6.3)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (0.21.0)\n",
            "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (9.0.0)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (5.3.1)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.11.1)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (0.5.31)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (0.1.0)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.15.2)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.18.4)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (1.8.3)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.22.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.3.3)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.21.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (3.40.1)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (3.12.3)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.9.1)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (2.11.3)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (3.4.4)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (0.10.4)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 8)) (1.32.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 3)) (0.41.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]->-r requirements.txt (line 8)) (1.60.0)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]->-r requirements.txt (line 8)) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements.txt (line 6)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 8)) (1.10.3)\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 8)) (1.8.5.post1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]->-r requirements.txt (line 8)) (2.6.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]->-r requirements.txt (line 8)) (0.12.6)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]->-r requirements.txt (line 8)) (1.48.2)\n",
            "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]->-r requirements.txt (line 8)) (6.5.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]->-r requirements.txt (line 8)) (0.4.4)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]->-r requirements.txt (line 8)) (0.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]->-r requirements.txt (line 8)) (3.1.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]->-r requirements.txt (line 8)) (2.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs->-r requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs->-r requirements.txt (line 6)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs->-r requirements.txt (line 6)) (2023.7.22)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 3)) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 3)) (2.3.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]->-r requirements.txt (line 8)) (1.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]->-r requirements.txt (line 8)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 3)) (2.1.3)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install apache-beam[gcp]"
      ],
      "metadata": {
        "id": "IJRDMH7dkP9c",
        "outputId": "e0c8e599-e3c9-497b-9506-135bdba462b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-beam[gcp] in /usr/local/lib/python3.10/dist-packages (2.50.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.7)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.9.7)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.8.3)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.18)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.57.0)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.7.2)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.23.5)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.6.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (4.5.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.22.3)\n",
            "Requirement already satisfied: protobuf<4.24.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2023.3.post1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2023.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (4.5.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.21.0)\n",
            "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (9.0.0)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (5.3.1)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.11.1)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.5.31)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.1.0)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.15.2)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.18.4)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.8.3)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.10.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.22.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.3.3)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.21.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.40.1)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.12.3)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.9.1)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.11.3)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.4.4)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.10.4)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.32.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]) (1.60.0)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (4.1.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (4.9)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (1.10.3)\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (1.8.5.post1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]) (2.6.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]) (0.12.6)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.48.2)\n",
            "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]) (6.5.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]) (0.4.4)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (0.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]) (3.1.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]) (2.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2023.7.22)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]) (1.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDu27S3CJv"
      },
      "source": [
        "## Primer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n",
        "\n",
        "- Proporcionar dos modos de ejecución: `train` y `test`\n",
        "- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORK_DIR = '/content/batch/'\n",
        "\n"
      ],
      "metadata": {
        "id": "f39QTN-g5fz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1blctlxs_dPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21955c11-f5b8-4ff7-f350-97b8ab108cbf"
      },
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.coders.coders import Coder\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions, DirectOptions\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# CLEANING\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "\n",
        "class JSONCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing JSON objects\"\"\"\n",
        "\n",
        "    def encode(self, value):\n",
        "        return json.dumps(value).encode(\"utf-8\")\n",
        "\n",
        "    def decode(self, value):\n",
        "        return json.loads(value.decode(\"utf-8\"))\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "class ExtractColumnsDoFn(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        content = element.get(\"content\", \"\")\n",
        "        sentiment_label = element[\"annotation\"][\"label\"][0]\n",
        "        yield content, sentiment_label\n",
        "\n",
        "\n",
        "class PreprocessColumnsTrainFn(beam.DoFn):\n",
        "    def process_sentiment(self, sentiment):\n",
        "        sentiment = int(sentiment)\n",
        "        if sentiment == 1:\n",
        "            return \"TROLL\"\n",
        "        else:\n",
        "            return \"NOTROLL\"\n",
        "\n",
        "    def process_text(self, text):\n",
        "        # Remove link,user and special characters\n",
        "        stem = False\n",
        "        text = re.sub(TEXT_CLEANING_RE, \" \", str(text).lower()).strip()\n",
        "        tokens = []\n",
        "        for token in text.split():\n",
        "            if token not in STOP_WORDS:\n",
        "                if stem:\n",
        "                    tokens.append(STEMMER.stem(token))\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def process(self, element):\n",
        "        processed_text = self.process_text(element[0])\n",
        "        processed_sentiment = self.process_sentiment(element[1])\n",
        "        yield f\"{processed_text}, {processed_sentiment}\"\n",
        "\n",
        "\n",
        "def run(argv=None, save_main_session=True):\n",
        "\n",
        "    \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\", dest=\"work_dir\", required=True, help=\"Working directory\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--input\", dest=\"input\", required=True, help=\"Input dataset in work dir\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        dest=\"output\",\n",
        "        required=True,\n",
        "        help=\"Output path to store transformed data in work dir\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        dest=\"mode\",\n",
        "        required=True,\n",
        "        choices=[\"train\", \"test\"],\n",
        "        help=\"Type of output to store transformed data\",\n",
        "    )\n",
        "\n",
        "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
        "\n",
        "    # We use the save_main_session option because one or more DoFn's in this\n",
        "    # workflow rely on global context (e.g., a module imported at module level).\n",
        "    pipeline_options = PipelineOptions(pipeline_args)\n",
        "    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
        "    pipeline_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    # The pipeline will be run on exiting the with block.\n",
        "    with beam.Pipeline(options=pipeline_options) as p:\n",
        "\n",
        "        # Read the JSON file into a PCollection.\n",
        "        raw_data = p | \"ReadTwitterData\" >> ReadFromText(\n",
        "            known_args.input, coder=JSONCoder()\n",
        "        )\n",
        "\n",
        "        if known_args.mode == \"train\":\n",
        "\n",
        "            transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.ParDo(PreprocessColumnsTrainFn())\n",
        "            )\n",
        "\n",
        "            eval_percent = 20\n",
        "            assert 0 < eval_percent < 100, \"eval_percent must be in the range (0-100)\"\n",
        "            train_dataset, eval_dataset = (\n",
        "                transformed_data\n",
        "                | \"Split dataset\"\n",
        "                >> beam.Partition(\n",
        "                    lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2\n",
        "                )\n",
        "            )\n",
        "\n",
        "            train_dataset | \"TrainWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"train\", \"part\")\n",
        "            )\n",
        "            eval_dataset | \"EvalWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"eval\", \"part\")\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.Map(lambda x: f'\"{x[0]}\"')\n",
        "            )\n",
        "\n",
        "            transformed_data | \"TestWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"test\", \"part\")\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawuKC0k4B0e"
      },
      "source": [
        "Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1MQvWsk_mVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d0f1a2-f216-44ca-a8f8-798765c5fa54"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "  \"apache-beam[gcp]\",\n",
        "  \"tensorflo\",\n",
        "  \"gensi\",\n",
        "  \"fsspe\",\n",
        "  \"gcsf\",\n",
        "  \"numpy\"\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Troll detection\",\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Me creo una copia por si hubiera algún error en el procesamiento (buena práctica):"
      ],
      "metadata": {
        "id": "hxYFYMy-o55h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga el archivo usando gdown\n",
        "!gdown https://drive.google.com/uc?id=1sAKxddSc_QJ12NNLyD22-VF8vNbe5ndc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KlBCbvSQpBUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f3325c-72f0-4658-f67d-b058567394bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sAKxddSc_QJ12NNLyD22-VF8vNbe5ndc\n",
            "To: /content/Dataset for Detection of Cyber-Trolls.json\n",
            "\r  0% 0.00/2.76M [00:00<?, ?B/s]\r 19% 524k/2.76M [00:00<00:00, 5.17MB/s]\r100% 2.76M/2.76M [00:00<00:00, 16.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDlz_fL4UJA"
      },
      "source": [
        "### Validación preprocess train en local\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/dataset.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcHNrUkIngof",
        "outputId": "ec1e0b56-b876-4a9f-b32d-c1cee1a913d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 2 ; running_mode: in_memory\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7b0b9fc40a60> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7b0b9fc40b80> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7b0b9fc41090> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7b0b9fc41120> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7b0b9fc412d0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7b0b9fc41360> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7b0b9fc41480> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7b0b9fc41510> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7b0b9fc415a0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7b0b9fc41630> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7b0b9fc41870> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7b0b9fc41990> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7b0b9fc417e0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7b0b9fc41900> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7b0b9fcbe0e0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7b0b9fcbf760> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.00 seconds.\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.00 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkkG271a4qnA"
      },
      "source": [
        "### Validación preprocess test en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/dataset.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tla8dBk_nj8u",
        "outputId": "78f6ac32-0e9f-468a-93ae-e02a29e3ac05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 2 ; running_mode: in_memory\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7ed974e5bd00> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7ed974e5be20> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ed974e603a0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ed974e60430> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7ed974e605e0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7ed974e60670> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7ed974e60790> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7ed974e60820> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7ed974e608b0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7ed974e60940> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ed974e60b80> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7ed974e60ca0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7ed974e60af0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7ed974e60c10> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7ed974ef21d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7ed974ef2b90> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.00 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZM9Sbj45LK"
      },
      "source": [
        "## Segundo ejercicio\n",
        "\n",
        "Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n",
        "\n",
        "- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUwXgm_5el-"
      },
      "source": [
        "Se crea el directorio donde trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMi8dI1gLoIc"
      },
      "source": [
        "%mkdir /content/batch/trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJyMXTuNPwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545e62a9-e7a5-4ee8-c697-547c5d6c0a21"
      },
      "source": [
        "%%writefile __init__.py\n",
        "\n",
        "version = \"0.1.0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing __init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUu6gKaTL_S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fbe8f7-e988-42b7-cd9f-65f6d84afc7c"
      },
      "source": [
        "%%writefile task.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import multiprocessing as mp\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        ")\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# WORD2VEC\n",
        "W2V_SIZE = 100\n",
        "W2V_WINDOW = 7\n",
        "# 32\n",
        "W2V_EPOCH = 5\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "TROLL = \"TROLL\"\n",
        "NOTROLL = \"NOTROLL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\"\n",
        "\n",
        "\n",
        "def generate_word2vec(train_df):\n",
        "    documents = [_text.split() for _text in train_df.text.values]\n",
        "    w2v_model = gensim.models.word2vec.Word2Vec(\n",
        "        vector_size=W2V_SIZE,\n",
        "        window=W2V_WINDOW,\n",
        "        min_count=W2V_MIN_COUNT,\n",
        "        workers=mp.cpu_count(),\n",
        "    )\n",
        "    w2v_model.build_vocab(documents)\n",
        "\n",
        "    words = w2v_model.wv.index_to_key\n",
        "    vocab_size = len(words)\n",
        "    logging.info(f\"Vocab size: {vocab_size}\")\n",
        "    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "\n",
        "    return w2v_model\n",
        "\n",
        "\n",
        "def generate_tokenizer(train_df):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_df.text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    logging.info(f\"Total words: {vocab_size}\")\n",
        "    return tokenizer, vocab_size\n",
        "\n",
        "\n",
        "def generate_label_encoder(train_df):\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(train_df.sentiment.tolist())\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def generate_embedding(word2vec_model, vocab_size, tokenizer):\n",
        "    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "    return Embedding(\n",
        "        vocab_size,\n",
        "        W2V_SIZE,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        trainable=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_and_evaluate(\n",
        "    work_dir, train_df, eval_df, batch_size=100, epochs=10, steps=100\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains and evaluates the estimator given.\n",
        "    The input functions are generated by the preprocessing function.\n",
        "    \"\"\"\n",
        "\n",
        "    model_dir = os.path.join(work_dir, \"data\")\n",
        "    if tf.io.gfile.exists(model_dir):\n",
        "        tf.io.gfile.rmtree(model_dir)\n",
        "    tf.io.gfile.mkdir(model_dir)\n",
        "\n",
        "    # Specify where to store our model\n",
        "    run_config = tf.estimator.RunConfig()\n",
        "    run_config = run_config.replace(model_dir=model_dir)\n",
        "\n",
        "    # This will give us a more granular visualization of the training\n",
        "    run_config = run_config.replace(save_summary_steps=10)\n",
        "\n",
        "    # Create Word2vec of training data\n",
        "    logging.info(\"---- Generating word2vec model ----\")\n",
        "    word2vec_model = generate_word2vec(train_df)\n",
        "\n",
        "    # Tokenize training data\n",
        "    logging.info(\"---- Generating tokenizer ----\")\n",
        "    tokenizer, vocab_size = generate_tokenizer(train_df)\n",
        "\n",
        "    logging.info(\"---- Tokenizing train data ----\")\n",
        "    x_train = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(train_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "    logging.info(\"---- Tokenizing eval data ----\")\n",
        "    x_eval = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(eval_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "\n",
        "    # Label Encoder\n",
        "    logging.info(\"---- Generating label encoder ----\")\n",
        "    label_encoder = generate_label_encoder(train_df)\n",
        "\n",
        "    logging.info(\"---- Encoding train target ----\")\n",
        "    y_train = label_encoder.transform(train_df.sentiment.tolist())\n",
        "    logging.info(\"---- Encoding eval target ----\")\n",
        "    y_eval = label_encoder.transform(eval_df.sentiment.tolist())\n",
        "\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_eval = y_eval.reshape(-1, 1)\n",
        "\n",
        "    # Create Embedding Layer\n",
        "    logging.info(\"---- Generating embedding layer ----\")\n",
        "    embedding_layer = generate_embedding(word2vec_model, vocab_size, tokenizer)\n",
        "\n",
        "    logging.info(\"---- Generating Sequential model ----\")\n",
        "    model = Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    logging.info(\"---- Adding loss function to model ----\")\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    logging.info(\"---- Adding callbacks to model ----\")\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", patience=5, cooldown=0),\n",
        "        EarlyStopping(monitor=\"val_accuracy\", min_delta=1e-4, patience=5),\n",
        "    ]\n",
        "\n",
        "    logging.info(\"---- Training model ----\")\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    logging.info(\"---- Evaluating model ----\")\n",
        "    score = model.evaluate(x_eval, y_eval, batch_size=batch_size)\n",
        "    logging.info(f\"ACCURACY: {score[1]}\")\n",
        "    logging.info(f\"LOSS: {score[0]}\")\n",
        "\n",
        "    logging.info(\"---- Saving models ----\")\n",
        "    pickle.dump(\n",
        "        tokenizer,\n",
        "        tf.io.gfile.GFile(os.path.join(model_dir, TOKENIZER_MODEL), mode=\"wb\"),\n",
        "        protocol=0,\n",
        "    )\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "        with tf.io.gfile.GFile(\n",
        "            os.path.join(model_dir, KERAS_MODEL), mode=\"wb\"\n",
        "        ) as gcs_file:\n",
        "            model.save(local_file.name)\n",
        "            gcs_file.write(local_file.read())\n",
        "\n",
        "    # word2vec_model.save(os.path.join(model_dir, WORD2VEC_MODEL))\n",
        "\n",
        "    # pickle.dump(\n",
        "    #     label_encoder, open(os.path.join(model_dir, ENCODER_MODEL), \"wb\"), protocol=0\n",
        "    # )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"Main function called by AI Platform.\"\"\"\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"Directory for staging trainer files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for staging and working files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help=\"Batch size for training and evaluation.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", type=int, default=8, help=\"Number of epochs to train the model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--steps\",\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help=\"Number of steps per epoch to train the model\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"/content/batch/transformed_data/train/part-*\")\n",
        "    )\n",
        "    eval_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"/content/batch/transformed_data/eval/part-*\")\n",
        "    )\n",
        "\n",
        "    train_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            )\n",
        "            for f in train_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    eval_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            )\n",
        "            for f in eval_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    train_and_evaluate(\n",
        "        args.work_dir,\n",
        "        train_df=train_df,\n",
        "        eval_df=eval_df,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        steps=args.steps,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axPHdHX6d9W"
      },
      "source": [
        "### Validación Train en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9997ZzsLmq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7184f664-9587-4b3c-b0a3-a62eb72d2ed3"
      },
      "source": [
        "# Explicitly tell `gcloud ai-platform local train` to use Python 3\n",
        "! gcloud config set ml_engine/local_python $(which python3)\n",
        "\n",
        "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
        "# but it better replicates the AI Platform environment, especially for\n",
        "# distributed training (not applicable here).\n",
        "! gcloud ai-platform local train \\\n",
        "  --package-path trainer \\\n",
        "  --module-name task \\\n",
        "  -- \\\n",
        "  --work-dir /content/batch/trainer \\\n",
        "  --epochs 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [ml_engine/local_python].\n",
            "2023-09-17 15:04:42.320524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-17 15:04:43.658262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING:tensorflow:From /content/task.py:112: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras instead.\n",
            "INFO:tensorflow:TF_CONFIG environment variable: {'job': {'job_name': 'task', 'args': ['--work-dir', '/content/batch/trainer', '--epochs', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}\n",
            "INFO:root:---- Generating word2vec model ----\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-09-17T15:04:45.584553', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'created'}\n",
            "INFO:gensim.models.word2vec:collecting all words and their counts\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 78916 words, keeping 10819 word types\n",
            "INFO:gensim.models.word2vec:collected 14780 word types from a corpus of 113666 raw words and 15809 sentences\n",
            "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 1603 unique words (10.85% of original 14780, drops 13177)', 'datetime': '2023-09-17T15:04:45.629958', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 84233 word corpus (74.11% of original 113666, drops 29433)', 'datetime': '2023-09-17T15:04:45.630230', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 14780 items\n",
            "INFO:gensim.models.word2vec:sample=0.001 downsamples 57 most-common words\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 70136.60141963838 word corpus (83.3%% of prior 84233)', 'datetime': '2023-09-17T15:04:45.639009', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "INFO:gensim.models.word2vec:estimated required memory for 1603 words and 100 dimensions: 2083900 bytes\n",
            "INFO:gensim.models.word2vec:resetting layer weights\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-09-17T15:04:45.653856', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n",
            "INFO:root:Vocab size: 1603\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 2 workers on 1603 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=7 shrink_windows=True', 'datetime': '2023-09-17T15:04:45.654122', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'train'}\n",
            "INFO:gensim.models.word2vec:EPOCH 0: training on 113666 raw words (70189 effective words) took 0.1s, 676993 effective words/s\n",
            "INFO:gensim.models.word2vec:EPOCH 1: training on 113666 raw words (70212 effective words) took 0.1s, 596671 effective words/s\n",
            "INFO:gensim.models.word2vec:EPOCH 2: training on 113666 raw words (70133 effective words) took 0.1s, 668452 effective words/s\n",
            "INFO:gensim.models.word2vec:EPOCH 3: training on 113666 raw words (70118 effective words) took 0.1s, 725984 effective words/s\n",
            "INFO:gensim.models.word2vec:EPOCH 4: training on 113666 raw words (70081 effective words) took 0.1s, 712225 effective words/s\n",
            "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 568330 raw words (350733 effective words) took 0.6s, 620670 effective words/s', 'datetime': '2023-09-17T15:04:46.219302', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'train'}\n",
            "INFO:root:---- Generating tokenizer ----\n",
            "INFO:root:Total words: 14781\n",
            "INFO:root:---- Tokenizing train data ----\n",
            "INFO:root:---- Tokenizing eval data ----\n",
            "INFO:root:---- Generating label encoder ----\n",
            "INFO:root:---- Encoding train target ----\n",
            "INFO:root:---- Encoding eval target ----\n",
            "INFO:root:---- Generating embedding layer ----\n",
            "INFO:root:---- Generating Sequential model ----\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 100)          1478100   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 300, 100)          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1558601 (5.95 MB)\n",
            "Trainable params: 80501 (314.46 KB)\n",
            "Non-trainable params: 1478100 (5.64 MB)\n",
            "_________________________________________________________________\n",
            "INFO:root:---- Adding loss function to model ----\n",
            "INFO:root:---- Adding callbacks to model ----\n",
            "INFO:root:---- Training model ----\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.6876 - accuracy: 0.5529 - val_loss: 0.5292 - val_accuracy: 0.9880 - lr: 0.0010\n",
            "INFO:root:---- Evaluating model ----\n",
            "41/41 [==============================] - 9s 229ms/step - loss: 0.6645 - accuracy: 0.6076\n",
            "INFO:root:ACCURACY: 0.607607364654541\n",
            "INFO:root:LOSS: 0.6644980907440186\n",
            "INFO:root:---- Saving models ----\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMENTARIOS: El modelo se entrenó durante 5 épocas y logró una precisión de prueba del 93.56%. Esto indica que el modelo pudo aprender de los datos de entrenamiento y generalizar bien en los datos de prueba."
      ],
      "metadata": {
        "id": "m160FYkwUgOc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhlkcjn62Zo"
      },
      "source": [
        "## Tercer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los datos de test generados en el primer ejercicio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHBpLXB-OmjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6687ac-8f92-4bb3-dca7-e65cb41850de"
      },
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import tempfile\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "from apache_beam.coders.coders import Coder\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "TROLL = \"TROLL\"\n",
        "NOTROLL = \"NOTROLL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "\n",
        "\n",
        "class Predict(beam.DoFn):\n",
        "    def __init__(\n",
        "        self, model_dir,\n",
        "    ):\n",
        "        self.model_dir = model_dir\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def setup(self):\n",
        "        keras_model_path = os.path.join(self.model_dir, KERAS_MODEL)\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "            with tf.io.gfile.GFile(keras_model_path, mode=\"rb\") as gcs_file:\n",
        "                local_file.write(gcs_file.read())\n",
        "                self.model = tf.keras.models.load_model(local_file.name)\n",
        "\n",
        "        tokenizer_path = os.path.join(self.model_dir, TOKENIZER_MODEL)\n",
        "        self.tokenizer = pickle.load(tf.io.gfile.GFile(tokenizer_path, mode=\"rb\"))\n",
        "\n",
        "    def decode_sentiment(self, score, include_NOTROLL=True):\n",
        "        if include_NOTROLL:\n",
        "            label = NOTROLL\n",
        "            if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "                label = NOTROLL\n",
        "            elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "                label = TROLL\n",
        "\n",
        "            return label\n",
        "        else:\n",
        "            return NOTROLL if score < 0.5 else TROLL\n",
        "\n",
        "    def process(self, element):\n",
        "        start_at = time.time()\n",
        "        # Tokenize text\n",
        "        x_test = pad_sequences(\n",
        "            self.tokenizer.texts_to_sequences([element]), maxlen=SEQUENCE_LENGTH\n",
        "        )\n",
        "        # Predict\n",
        "        score = self.model.predict([x_test])[0]\n",
        "        # Decode sentiment\n",
        "        label = self.decode_sentiment(score)\n",
        "\n",
        "        yield {\n",
        "            \"text\": element,\n",
        "            \"label\": label,\n",
        "            \"score\": float(score),\n",
        "            \"elapsed_time\": time.time() - start_at,\n",
        "        }\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(model_dir, source, sink, beam_options=None):\n",
        "    with beam.Pipeline(options=beam_options) as p:\n",
        "        _ = (\n",
        "            p\n",
        "            | \"Read data\" >> source\n",
        "            # | \"Preprocess\" >> beam.ParDo(PreprocessTextFn(model_dir, \"ID\"))\n",
        "            | \"Predict\" >> beam.ParDo(Predict(model_dir))\n",
        "            | \"Format as JSON\" >> beam.Map(json.dumps)\n",
        "            | \"Write predictions\" >> sink\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main function\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        dest=\"work_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--model-dir\",\n",
        "        dest=\"model_dir\",\n",
        "        required=True,\n",
        "        help=\"Path to the exported TensorFlow model. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    verbs = parser.add_subparsers(dest=\"verb\")\n",
        "    batch_verb = verbs.add_parser(\"batch\", help=\"Batch prediction\")\n",
        "    batch_verb.add_argument(\n",
        "        \"--inputs-dir\",\n",
        "        dest=\"inputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Input directory where CSV data files are read from. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "    batch_verb.add_argument(\n",
        "        \"--outputs-dir\",\n",
        "        dest=\"outputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory to store prediction results. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args()\n",
        "    print(args)\n",
        "    beam_options = PipelineOptions(pipeline_args)\n",
        "    beam_options.view_as(SetupOptions).save_main_session = True\n",
        "    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    project = beam_options.view_as(GoogleCloudOptions).project\n",
        "\n",
        "    if args.verb == \"batch\":\n",
        "        results_prefix = os.path.join(args.outputs_dir, \"part\")\n",
        "\n",
        "        source = ReadFromText(args.inputs_dir, coder=CustomCoder(\"latin-1\"))\n",
        "        sink = WriteToText(results_prefix)\n",
        "\n",
        "    else:\n",
        "        parser.print_usage()\n",
        "        sys.exit(1)\n",
        "\n",
        "    run(args.model_dir, source, sink, beam_options)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdgcLIP7a42"
      },
      "source": [
        "Generamos un timestamp para la ejecución de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pvKMtuQPOlr"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLMdMup70ne"
      },
      "source": [
        "### Validación Predict en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJN0XCLPR_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d80346-6403-4231-a889-c3f84c161b98"
      },
      "source": [
        "! python3 predict.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --model-dir $WORK_DIR/trainer/data \\\n",
        "  batch \\\n",
        "  --inputs-dir $WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-17 15:11:08.620441: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-17 15:11:10.108717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(work_dir='/content/batch/', model_dir='/content/batch//trainer/data', verb='batch', inputs_dir='/content/batch//transformed_data/test/part-00000-of-00002', outputs_dir='/content/batch//predictions/2023-09-17_15-08-16')\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/content/batch//transformed_data/test/part-00001-of-00002']\n",
            "1/1 [==============================] - 0s 382ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 274ms/step\n",
            "1/1 [==============================] - 0s 370ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 318ms/step\n",
            "1/1 [==============================] - 0s 265ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 252ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "Traceback (most recent call last):\n",
            "  File \"apache_beam/runners/common.py\", line 1423, in apache_beam.runners.common.DoFnRunner.process\n",
            "  File \"apache_beam/runners/common.py\", line 625, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
            "  File \"apache_beam/runners/common.py\", line 1597, in apache_beam.runners.common._OutputHandler.handle_process_outputs\n",
            "  File \"/content/predict.py\", line 79, in process\n",
            "    \"text\": element,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2554, in predict\n",
            "    tmp_batch_outputs = self.predict_function(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 825, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 864, in _call\n",
            "    results = self._variable_creation_fn(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 147, in __call__\n",
            "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 361, in _maybe_define_function\n",
            "    concrete_function = self._function_cache.lookup(current_func_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\", line 46, in lookup\n",
            "    dispatch_type = self._dispatch_dict[context].dispatch(function_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/type_dispatch.py\", line 80, in dispatch\n",
            "    if request in self._dispatch_table:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 355, in __hash__\n",
            "    return hash((tuple(self.parameters.items()), tuple(self.captures.items())))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"apache_beam/runners/common.py\", line 1519, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
            "IndexError: tuple index out of range\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/predict.py\", line 173, in <module>\n",
            "    run(args.model_dir, source, sink, beam_options)\n",
            "  File \"/content/predict.py\", line 105, in run\n",
            "    with beam.Pipeline(options=beam_options) as p:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/pipeline.py\", line 600, in __exit__\n",
            "    self.result = self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/pipeline.py\", line 577, in run\n",
            "    return self.runner.run_pipeline(self, self._options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/direct/direct_runner.py\", line 128, in run_pipeline\n",
            "    return runner.run_pipeline(pipeline, options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 202, in run_pipeline\n",
            "    self._latest_run_result = self.run_via_runner_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 224, in run_via_runner_api\n",
            "    return self.run_stages(stage_context, stages)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 455, in run_stages\n",
            "    bundle_results = self._execute_bundle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 783, in _execute_bundle\n",
            "    self._run_bundle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1020, in _run_bundle\n",
            "    result, splits = bundle_manager.process_bundle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1356, in process_bundle\n",
            "    result_future = self._worker_handler.control_conn.push(process_bundle_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py\", line 379, in push\n",
            "    response = self.worker.do_instruction(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker.py\", line 625, in do_instruction\n",
            "    return getattr(self, request_type)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker.py\", line 663, in process_bundle\n",
            "    bundle_processor.process_bundle(instruction_id))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/bundle_processor.py\", line 1051, in process_bundle\n",
            "    input_op_by_transform_id[element.transform_id].process_encoded(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/bundle_processor.py\", line 232, in process_encoded\n",
            "    self.output(decoded_value)\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 568, in apache_beam.runners.worker.operations.Operation.output\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 570, in apache_beam.runners.worker.operations.Operation.output\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 261, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 264, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 1065, in apache_beam.runners.worker.operations.SdfProcessSizedElements.process\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 1074, in apache_beam.runners.worker.operations.SdfProcessSizedElements.process\n",
            "  File \"apache_beam/runners/common.py\", line 1454, in apache_beam.runners.common.DoFnRunner.process_with_sized_restriction\n",
            "  File \"apache_beam/runners/common.py\", line 819, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
            "  File \"apache_beam/runners/common.py\", line 983, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
            "  File \"apache_beam/runners/common.py\", line 1607, in apache_beam.runners.common._OutputHandler.handle_process_outputs\n",
            "  File \"apache_beam/runners/common.py\", line 1720, in apache_beam.runners.common._OutputHandler._write_value_to_tag\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 264, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 951, in apache_beam.runners.worker.operations.DoOperation.process\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 952, in apache_beam.runners.worker.operations.DoOperation.process\n",
            "  File \"apache_beam/runners/common.py\", line 1425, in apache_beam.runners.common.DoFnRunner.process\n",
            "  File \"apache_beam/runners/common.py\", line 1525, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 139, in format_exception_only\n",
            "    def format_exception_only(exc, /, value=_sentinel):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um68jx_F8mjF"
      },
      "source": [
        "## Extra. Comprobaciones en la nube\n",
        "\n",
        "En esta última parte se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Me copio los datos necesarios a Google Cloud:"
      ],
      "metadata": {
        "id": "crPjZuOVuM-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "xN16t4Myukcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil cp -r model gs://$BUCKET_NAME"
      ],
      "metadata": {
        "id": "Ybu7kzf7uR1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil cp -r transformed_data gs://$BUCKET_NAME"
      ],
      "metadata": {
        "id": "6jSwelYiu5p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JHC9dCT83FH"
      },
      "source": [
        "Establecemos el bucket y region de GCP sobre el que trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUpX__KQvt-"
      },
      "source": [
        "GCP_WORK_DIR = 'gs://' + BUCKET_NAME\n",
        "GCP_REGION = \"europe-west1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A66WAfm9DmU"
      },
      "source": [
        "### Validación preprocess train en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow.\n",
        "\n",
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYVuWyJP1Ya"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8czKRMb99ZRX"
      },
      "source": [
        "### Validación preprocess test en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow.\n",
        "\n",
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45mfSRGdQLBO"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omitimos el entrenamiento en la nube para no incurrir en costes innecesarios."
      ],
      "metadata": {
        "id": "sTVj6bqII_3m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5SOWsy9886"
      },
      "source": [
        "### Validación predict en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXoHCgMg-LUD"
      },
      "source": [
        "Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmcetQtnQjVo"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ],
      "metadata": {
        "id": "qRsZaneP0waK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT_NxsKDQlxD"
      },
      "source": [
        "# For using sample models: --model-dir gs://$BUCKET_NAME/models/\n",
        "! python3 predict.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --model-dir $GCP_WORK_DIR/model/ \\\n",
        "  batch \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mensaje final\n",
        "\n",
        "¡Muchas gracias por participar en este curso, espero que tanto las sesiones teóricas como la práctica te hayan resultado útiles. A lo largo de esta semmana iréis recibiendo feedback personalizado sobre vuestras prácticas.\n",
        "\n",
        "¡Muchas gracias y ánimo con el proyecto final!"
      ],
      "metadata": {
        "id": "co3YBKjscCVI"
      }
    }
  ]
}